{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlHxi3dicoHz2pjz+KCnX7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymoslem/Adaptive-MT-LLM-Fine-tuning/blob/main/Convert-Mistral-Finetuned-CTranslate2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert a fine-tuned model from Transformers to CTranslate2\n",
        "This notebook is part of the repository [Adaptive-MT-LLM-Fine-tuning](https://github.com/ymoslem/Adaptive-MT-LLM-Fine-tuning)."
      ],
      "metadata": {
        "id": "w5-ecMbC_nCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load a model fine-tuned with PEFT"
      ],
      "metadata": {
        "id": "ECx1si54GXAt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXyeRsQR7asx"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers peft accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lhat /content/drive/MyDrive/data/spanish/mistral_finetuning*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L9-miSvxPNM",
        "outputId": "328caeaa-01be-4122-fc55-0ea871b8adf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data/spanish/mistral_finetuning_v2:\n",
            "total 20K\n",
            "drwx------ 2 root root 4.0K Nov 26 02:12 checkpoint-608\n",
            "drwx------ 2 root root 4.0K Nov 26 01:57 checkpoint-456\n",
            "drwx------ 2 root root 4.0K Nov 26 01:41 checkpoint-304\n",
            "drwx------ 2 root root 4.0K Nov 26 01:25 checkpoint-152\n",
            "drwx------ 2 root root 4.0K Nov 26 01:09 runs\n",
            "\n",
            "/content/drive/MyDrive/data/spanish/mistral_finetuning:\n",
            "total 8.0K\n",
            "drwx------ 2 root root 4.0K Nov 25 01:01 checkpoint-121\n",
            "drwx------ 2 root root 4.0K Nov 25 00:44 runs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Miatral one epoch\n",
        "adapter_path = \"/content/drive/MyDrive/data/spanish/mistral_finetuning/checkpoint-121\"\n",
        "\n",
        "# Mistral 4 epochs\n",
        "# adapter_path = \"/content/drive/MyDrive/data/spanish/mistral_finetuning_v2/checkpoint-608\"\n",
        "\n",
        "cache_dir = \"/content/drive/MyDrive/models/\"\n",
        "# offload_folder = \"\"\n",
        "\n",
        "peft_model_id = adapter_path\n",
        "peftconfig = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(peftconfig.base_model_name_or_path,\n",
        "                                             device_map = \"auto\",\n",
        "                                             # offload_folder = offload_folder,\n",
        "                                             cache_dir = cache_dir\n",
        "                                            )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peftconfig.base_model_name_or_path)\n",
        "\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "\n",
        "print(\"Peft model loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195,
          "referenced_widgets": [
            "36bf89e06b964c9689481d22c476c086",
            "2d45bc06b26145d1aadc206352e9076c",
            "7a57f83bdb6d426f9c126680e30dc677",
            "50cc7a604f7245efabca85ec44196d12",
            "e4751704f50b4f62ae49b73ba5178c85",
            "4bdd71e32e2349559fa5c0cbe89198a8",
            "20a5033df0c34d65833de8e77b5068e9",
            "0830ad95f9f74b7e9b5f305018d2fa42",
            "79d2b60c97294e79a452ad19c14f2180",
            "d2b65fab549d48959e5eb22ab600b635",
            "7cccd6e55b394992b58e256ab900df2c",
            "54e79740b81146868c28eb7d7546a91e",
            "23a66b3d05cd4adebbdd10cb55cae8ee",
            "99667f151a4d44628e3389dbcb8254a3",
            "01de8575f349491b8dddfee235c43a6d",
            "c2f591ff66be49e3a50b40125d52579d",
            "8da9fa56b45647a9824bbdaf8f3b35b1",
            "8724bd9e08ac40f9bb6f91d3774d565c",
            "8c1b7b7f057a4ed483163ce8ed12b9f9",
            "66a0f57d85c0441fad43b229c307cb9b",
            "3d87b466bc3a4fe89616faf1889e5369",
            "a66e8d64991f40ce8686a7c8621fd238",
            "b3f004260d3f431b8127f8fe908885fc",
            "3ee2a11b66e641519c142ad2b9aad201",
            "ac6fdaeeeafb44f881247788f61b4f9c",
            "00ee3e3920694b83bb7958a4b1c3da05",
            "9bcfe85dfd14475fae0ee1abe36be71e",
            "86f0e3d471924f359e164558da6dbab1",
            "a78bec89638741a8bc6aae2d27f45035",
            "7f6cd672d3a84033832c7afcab63587a",
            "2fe67440a86f4781833d87224b5b5785",
            "0d04847a17844356927b0b29895222b1",
            "bec426755e934b6f9a8570d226fa7914",
            "c61b4ef287384ab495d9d731bec857c8",
            "a58b9ebd785543169a8808a6d602e2ac",
            "7460546e62db4ca79dd31a4005d8d87b",
            "c8f75fe9b92e453d8cb3170cacfb8571",
            "28bf50998bd84073907f7cc0702489b4",
            "843eb23df12c4446ba568228467ea485",
            "ea7df0b53204406f9a9b5bf6b04c8941",
            "3f64148bd90d48739c021381909ed496",
            "63568b4d59c5453c97339e9c57141238",
            "df0501afd3dc49b9a364e7d25c82f279",
            "c691300613444ae9886e09630d687915",
            "839e2214ddb94f0fae43dedd26782003",
            "4f28f49aa7e142199e05a6beb0762afb",
            "4b348c3290b54dc0a83654a60db000a4",
            "cb4c296b65f24cf39da5e5a3c7452a15",
            "61099ac51183469ea95deea9189a77c0",
            "8ca67d65de0844f188f284c115b31f97",
            "3a9db69665f64f5890600c28ab40294f",
            "3f5f92787b904eb0a0b9056ba4072f9b",
            "3db9bde51a90466b82d17c4ea23f55e9",
            "06116e2fd4884092a7a36f24996a6d8f",
            "996b580b3da64c8f93a9387804692930"
          ]
        },
        "id": "5Dz4anFo9II9",
        "outputId": "0de84f3f-b40f-4a91-aac5-a881960027dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36bf89e06b964c9689481d22c476c086",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/966 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54e79740b81146868c28eb7d7546a91e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3f004260d3f431b8127f8fe908885fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c61b4ef287384ab495d9d731bec857c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "839e2214ddb94f0fae43dedd26782003"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peft model loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "shared_drive = userdata.get(\"shared_drive\")\n",
        "\n",
        "final_model_path = os.path.join(shared_drive, \"models/Mistral-7B-finetuned-v1-25Nov\")\n",
        "\n",
        "merged_model = model.merge_and_unload()\n",
        "\n",
        "merged_model.save_pretrained(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)"
      ],
      "metadata": {
        "id": "-D387w-J_O-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert to CTranslate2"
      ],
      "metadata": {
        "id": "z2BXdVowGPJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install ctranslate2 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dD_Fg-bGOqE",
        "outputId": "6adb45b7-9c95-4503-c033-3ace5a07ad75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "shared_drive = userdata.get(\"shared_drive\")\n",
        "\n",
        "directory = os.path.join(shared_drive, \"models\")\n",
        "\n",
        "os.chdir(directory)\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "YoISl2qTHhCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ct2_model_dir = \"ct2-mistral-finetuned-v1-25Nov\"  # 1 epoch\n",
        "# ct2_model_dir = \"ct2-mistral-finetuned-v2-26Nov\"  # 4 epochs"
      ],
      "metadata": {
        "id": "SJU2glhQziv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ct2-transformers-converter --model $final_model_path --output_dir $ct2_model_dir --quantization float16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGB-wrbWGfe0",
        "outputId": "1532ca78-b11e-4824-e450-8ade20084544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint shards: 100% 6/6 [01:29<00:00, 14.95s/it]\n"
          ]
        }
      ]
    }
  ]
}